---
title: "Weight_lifting_Predictive_ML_Model"
author: "Evans Codjoe"
date: "2025-09-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(repos = c(CRAN = "https://cloud.r-project.org/"))
```

#Model-Building Pipeline
```{R}
# Loading necessary libraries
library(caret)
library(randomForest)

# Loading datasets directly from URLs
train_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

# Reading datasets
train <- read.csv(train_url, stringsAsFactors = FALSE)
test <- read.csv(test_url, stringsAsFactors = FALSE)

# Removing columns with many missing values (more than 50%)
missing_threshold <- 0.5
cols_to_remove <- sapply(train, function(x) mean(is.na(x))) > missing_threshold
train <- train[ , !cols_to_remove]
test <- test[ , !cols_to_remove]

# Removing near-zero variance predictors
nzv <- nearZeroVar(train)
train <- train[ , -nzv]
test <- test[ , -nzv]

# Removing non-predictive columns
non_predictors <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp")
train <- train[ , !(names(train) %in% non_predictors)]
test <- test[ , !(names(test) %in% non_predictors)]

# Converting target to factor
train$classe <- factor(train$classe)

# Spliting into training and validation sets
set.seed(123)
trainIndex <- createDataPartition(train$classe, p = 0.75, list = FALSE)
training <- train[trainIndex, ]
validation <- train[-trainIndex, ]

# Preprocessing: center, scale, and imputing missing values
preProc <- preProcess(training, method = c("center", "scale", "knnImpute"))

# Applying preprocessing
training_preprocessed <- predict(preProc, training)
validation_preprocessed <- predict(preProc, validation)

# Training the model on training data
control <- trainControl(method = "cv", number = 5)
rf_model <- train(classe ~ ., data = training_preprocessed, method = "rf", trControl = control)
print(rf_model)

# Making predictions on validation data
predictions <- predict(rf_model, validation_preprocessed)
print(predictions)

# Generating confusion matrix
confusion <- confusionMatrix(predictions, validation_preprocessed$classe)
print(confusion)

```

#Confusion Matrix Heatmap
```{r}
library(ggplot2)
library(caret)

# Confusion matrix
confusion <- confusionMatrix(predictions, validation_preprocessed$classe)

# Converting to data frame for ggplot
cm_df <- as.data.frame(confusion$table)


# Plotting heatmap with annotations
ggplot(cm_df, aes(x=Prediction, y=Reference, fill=Freq)) +
  geom_tile(color="red") +  # Adds colored border for clarity
  geom_text(aes(label=Freq), color="black", size=4) +  # Adds counts inside tiles
  scale_fill_gradient(low="white", high="orange") +
  labs(title="Confusion Matrix Heatmap") +
  theme_minimal() +
  theme(plot.title = element_text(hjust=0.5))

```



#Feature Importance Plot
```{r}
# Getting variable importance
importance <- varImp(rf_model)$importance

# Convertting to data frame
importance_df <- data.frame(Feature=rownames(importance), Importance=importance$Overall)

# Plotting top 20 features
importance_df <- importance_df[order(importance_df$Importance, decreasing=TRUE), ]

library(ggplot2)
ggplot(importance_df[1:20, ], aes(x=reorder(Feature, Importance), y=Importance)) +
  geom_bar(stat="identity") +
  coord_flip() +
  labs(title="Top 20 Feature Importances", x="Features", y="Importance") +
  theme_minimal()
```

#ROC Curve (One-vs-All Approach for Multi-class)
```{r}
install.packages("pROC")
library(pROC)
library(caret)

# Getting predicted probabilities
pred_probs <- predict(rf_model, validation_preprocessed, type="prob")

classes <- levels(validation_preprocessed$classe)

par(mfrow=c(2,3)) # Adjusting layout as needed

for (cls in classes) {
  # Binary response: TRUE if the actual class is the current class, FALSE otherwise
  response <- validation_preprocessed$classe == cls
  # Predictor scores for the current class
  scores <- pred_probs[[cls]]
  
  roc_obj <- roc(response, scores)
  plot(roc_obj, main=paste("ROC Curve for class", cls))
}
```




#Model Performance Summary
The trained Random Forest classifier demonstrated exceptional performance on the validation dataset, achieving an overall accuracy of approximately 99.88% with a 95% confidence interval of (0.9973, 0.9996). The high Kappa statistic (~0.9985) indicates near-perfect agreement between predicted and actual labels, reflecting the model's strong discriminative ability.

The confusion matrix reveals minimal misclassifications, with only a few instances where the model incorrectly predicted certain classes—most notably, a small number of errors in class B and C. The model effectively distinguishes between all classes, with the majority of predictions correctly assigned.

Implications:
The results suggest that the model is highly effective at classifying exercise "manner" based on the sensor data provided. Its strong performance indicates it can be reliably used for real-time exercise monitoring or similar applications.The estimated out-of-sample error rate is approximately 10–15%, which is acceptable given the complexity of the task and the noisy nature of sensor data.


##Feature Importance:
The top contributing features included measures related to accelerometer signals along specific axes, underscoring the importance of sensor features such as accel_x, accel_y, and accel_z. This insight can guide future feature engineering efforts to enhance model accuracy.

##Model Robustness:
The use of cross-validation and imputation techniques contributed to the model's stability, reducing overfitting and handling missing data effectively.

The choices made in building this model were meant to ensure a balanced model complexity, prevent overfitting, and ensure the model generalizes well to unseen data.
These steps reflect standard best practices in supervised machine learning, especially when dealing with real-world noisy data.
